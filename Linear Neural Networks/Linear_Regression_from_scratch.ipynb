{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression from scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSNZIKsS6X3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mxnet as mx\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHTtgbtx0Ak",
        "colab_type": "text"
      },
      "source": [
        "Class for implementation of linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAuXI2657VwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegression():\n",
        "\n",
        "  def __init__(self, weights, bias ):\n",
        "\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "\n",
        "  def synthetic_data(self, w, b, size):\n",
        "    \"\"\"\n",
        "    weights: synethic_weight nd array\n",
        "\n",
        "    bias: fixed value \n",
        "\n",
        "    size: the size of data set we want to create\n",
        "\n",
        "    returns genearted data\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Genearte X with normal distribution\n",
        "\n",
        "    X = mx.nd.normal(0,  0.1, (size, len(w)),\n",
        "                     ctx = mx.gpu())\n",
        "    \n",
        "    \n",
        "    y = mx.nd.dot(X, w) + b # add bias\n",
        "\n",
        "    #adding additional noise which is normally distribued centered at 0 and variance : 0.1\n",
        "\n",
        "    y = y + mx.nd.normal(0, 0.1, y.shape,\n",
        "                         ctx = mx.gpu()) \n",
        "\n",
        "    return X,y\n",
        "\n",
        "  def data_iter(self , batch_size, features, labels):\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size:  min batch size\n",
        "\n",
        "    features: X matrix\n",
        "\n",
        "    labels: y\n",
        "\n",
        "    returns: X,y pairs  in mini batches \n",
        "    \"\"\"\n",
        "\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    random.shuffle(indices) # shuffle indices to pick randomly for each batch\n",
        "    batch_indcies = mx.nd.array([], mx.gpu())\n",
        "\n",
        "\n",
        "    #  create batches of batch_size  \n",
        "    for i in range(0, num_examples, batch_size):\n",
        "\n",
        "      batch_indices = mx.nd.array( indices[i: min(i+batch_size, num_examples)], mx.gpu() )\n",
        "\n",
        "      yield features[batch_indices], labels[batch_indices]\n",
        "\n",
        "  def linreg(self, parameters, X):\n",
        "\n",
        "    \"\"\"\n",
        "    Linear regression equation\n",
        "\n",
        "\n",
        "    X : m x n matrix\n",
        "\n",
        "    w : n x n matrix\n",
        "\n",
        "    b : bias term\n",
        "    \"\"\"\n",
        "\n",
        "    y = mx.nd.dot(X, parameters[0]) + parameters[1] \n",
        "\n",
        "    return y\n",
        "\n",
        "  def squared_loss( self, y_hat, y):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    y_hat : estimate of y\n",
        "\n",
        "    y : actual value of y\n",
        "\n",
        "    returns: squared loss \n",
        "    \"\"\"\n",
        "\n",
        "    return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
        "\n",
        "  def sgd(self,parameters, lr, batch_size):\n",
        "\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "\n",
        "    parameters: model parameters\n",
        "    lr : learning rate\n",
        "    batch_Size : mini batch size\n",
        "    returns: updated parameters  \n",
        "    \"\"\"\n",
        "    for param in parameters:\n",
        "      param[:] = param - lr * param.grad / batch_size\n",
        "    # self.weights = self.weights - lr*self.weights / batch_size\n",
        "    # self.bias = self.bias - lr*self.bias /batch_size \n",
        "    \n",
        "\n",
        "\n",
        "  def train(self, epochs, features, labels, batch_size, lr):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    self.weights.attach_grad()\n",
        "    self.bias.attach_grad()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "\n",
        "      for X,y in self.data_iter(batch_size, features, labels):\n",
        "\n",
        "\n",
        "        with mx.autograd.record():\n",
        "          loss_batch = self.squared_loss(self.linreg([self.weights, self.bias], X), y)\n",
        "\n",
        "        loss_batch.backward()\n",
        "\n",
        "        \n",
        "\n",
        "        self.sgd([self.weights, self.bias], lr, batch_size)\n",
        "\n",
        "      train_l = self.squared_loss(lrobj.linreg([self.weights, self.bias], features), labels)\n",
        "\n",
        "      print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))\n",
        "\n",
        "      print(self.weights)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrezs83vx61C",
        "colab_type": "text"
      },
      "source": [
        "Training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUkC3LU0-yCC",
        "colab_type": "code",
        "outputId": "4cd9073f-4029-4c39-94d5-f17d6acb8315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "### Initialize model parameters\n",
        "w = mx.nd.random.normal(0, 0.01, (2, 1), ctx = mx.gpu()) \n",
        "b = mx.nd.zeros(1, ctx = mx.gpu())\n",
        "\n",
        "lrobj = LinearRegression(w,b)\n",
        "### genearte data\n",
        " \n",
        "true_w = mx.nd.array([2, -3.4], mx.gpu())\n",
        "true_b = 4.2\n",
        "data = lrobj.synthetic_data( true_w, true_b, 1000)\n",
        "\n",
        "### Allocate space to gradients of w and b\n",
        "\n",
        "\n",
        "### defining epochs, learning rates and batch sizes\n",
        "\n",
        "epochs = 100\n",
        "lr = 0.1\n",
        "batch_size = 10\n",
        "features = data[0]\n",
        "labels = data[1]\n",
        "\n",
        "lrobj.train( epochs, features, labels, batch_size, lr)\n",
        "\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "\n",
        "#   print(w,b)\n",
        "\n",
        "#   for X,y in lrobj.data_iter(batch_size, features, labels):\n",
        "\n",
        "\n",
        "#     with mx.autograd.record():\n",
        "#       loss_batch = lrobj.squared_loss(lrobj.linreg(X,w,b), y)\n",
        "\n",
        "#     loss_batch.backward()\n",
        "\n",
        "    \n",
        "\n",
        "#     lrobj.sgd([w,b], lr, batch_size)\n",
        "\n",
        "#   train_l = lrobj.squared_loss(lrobj.linreg(features, w, b), labels)\n",
        "\n",
        "#   print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.064864\n",
            "\n",
            "[[ 0.20256732]\n",
            " [-0.34180346]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 2, loss 0.055267\n",
            "\n",
            "[[ 0.36228392]\n",
            " [-0.61755323]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 3, loss 0.046140\n",
            "\n",
            "[[ 0.50766987]\n",
            " [-0.869848  ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 4, loss 0.039041\n",
            "\n",
            "[[ 0.64071906]\n",
            " [-1.0993583 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 5, loss 0.033326\n",
            "\n",
            "[[ 0.76151705]\n",
            " [-1.3079939 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 6, loss 0.028491\n",
            "\n",
            "[[ 0.8719064]\n",
            " [-1.4974056]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 7, loss 0.024482\n",
            "\n",
            "[[ 0.9729746]\n",
            " [-1.6694366]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 8, loss 0.021214\n",
            "\n",
            "[[ 1.0642432]\n",
            " [-1.8261853]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 9, loss 0.018600\n",
            "\n",
            "[[ 1.1486788]\n",
            " [-1.9683475]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 10, loss 0.016192\n",
            "\n",
            "[[ 1.2251072]\n",
            " [-2.0972092]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 11, loss 0.014337\n",
            "\n",
            "[[ 1.2950623]\n",
            " [-2.2145708]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 12, loss 0.012820\n",
            "\n",
            "[[ 1.3590178]\n",
            " [-2.3207161]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 13, loss 0.011595\n",
            "\n",
            "[[ 1.4174912]\n",
            " [-2.4171684]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 14, loss 0.010502\n",
            "\n",
            "[[ 1.4709553]\n",
            " [-2.504767 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 15, loss 0.009586\n",
            "\n",
            "[[ 1.5195575]\n",
            " [-2.5844276]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 16, loss 0.008937\n",
            "\n",
            "[[ 1.564018 ]\n",
            " [-2.6571908]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 17, loss 0.008262\n",
            "\n",
            "[[ 1.6052283]\n",
            " [-2.7232666]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 18, loss 0.007813\n",
            "\n",
            "[[ 1.642037 ]\n",
            " [-2.7828324]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 19, loss 0.007355\n",
            "\n",
            "[[ 1.6756419]\n",
            " [-2.8377433]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 20, loss 0.007016\n",
            "\n",
            "[[ 1.7066989]\n",
            " [-2.8870265]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 21, loss 0.006733\n",
            "\n",
            "[[ 1.7347199]\n",
            " [-2.9323177]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 22, loss 0.006508\n",
            "\n",
            "[[ 1.7606001]\n",
            " [-2.9729486]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 23, loss 0.006417\n",
            "\n",
            "[[ 1.7835146]\n",
            " [-3.0101397]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 24, loss 0.006205\n",
            "\n",
            "[[ 1.8051885]\n",
            " [-3.0441403]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 25, loss 0.006053\n",
            "\n",
            "[[ 1.8249409]\n",
            " [-3.0744076]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 26, loss 0.005913\n",
            "\n",
            "[[ 1.8427407]\n",
            " [-3.1026359]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 27, loss 0.005839\n",
            "\n",
            "[[ 1.8592731]\n",
            " [-3.1275656]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 28, loss 0.005749\n",
            "\n",
            "[[ 1.874266 ]\n",
            " [-3.1505659]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 29, loss 0.005714\n",
            "\n",
            "[[ 1.887967 ]\n",
            " [-3.1713836]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 30, loss 0.005652\n",
            "\n",
            "[[ 1.8999064]\n",
            " [-3.1899567]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 31, loss 0.005590\n",
            "\n",
            "[[ 1.911074 ]\n",
            " [-3.2070754]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 32, loss 0.005567\n",
            "\n",
            "[[ 1.9213223]\n",
            " [-3.2230704]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 33, loss 0.005537\n",
            "\n",
            "[[ 1.9305723]\n",
            " [-3.2372713]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 34, loss 0.005499\n",
            "\n",
            "[[ 1.939167 ]\n",
            " [-3.2506096]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 35, loss 0.005494\n",
            "\n",
            "[[ 1.9464884]\n",
            " [-3.2625113]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 36, loss 0.005463\n",
            "\n",
            "[[ 1.9533643]\n",
            " [-3.2729409]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 37, loss 0.005452\n",
            "\n",
            "[[ 1.959903 ]\n",
            " [-3.2826385]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 38, loss 0.005438\n",
            "\n",
            "[[ 1.9655538]\n",
            " [-3.2913055]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 39, loss 0.005428\n",
            "\n",
            "[[ 1.9711261]\n",
            " [-3.2992826]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 40, loss 0.005440\n",
            "\n",
            "[[ 1.9759701]\n",
            " [-3.306634 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 41, loss 0.005469\n",
            "\n",
            "[[ 1.980332]\n",
            " [-3.313694]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 42, loss 0.005427\n",
            "\n",
            "[[ 1.9848189]\n",
            " [-3.3198957]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 43, loss 0.005438\n",
            "\n",
            "[[ 1.9890089]\n",
            " [-3.3250659]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 44, loss 0.005403\n",
            "\n",
            "[[ 1.9921116]\n",
            " [-3.3298342]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 45, loss 0.005404\n",
            "\n",
            "[[ 1.9953923]\n",
            " [-3.334368 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 46, loss 0.005410\n",
            "\n",
            "[[ 1.9981263]\n",
            " [-3.3385098]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 47, loss 0.005528\n",
            "\n",
            "[[ 2.0008254]\n",
            " [-3.3424253]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 48, loss 0.005391\n",
            "\n",
            "[[ 2.0029705]\n",
            " [-3.3454924]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 49, loss 0.005391\n",
            "\n",
            "[[ 2.0050004]\n",
            " [-3.3486426]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 50, loss 0.005402\n",
            "\n",
            "[[ 2.0068173]\n",
            " [-3.3515742]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 51, loss 0.005437\n",
            "\n",
            "[[ 2.0084805]\n",
            " [-3.3541229]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 52, loss 0.005392\n",
            "\n",
            "[[ 2.009919]\n",
            " [-3.356735]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 53, loss 0.005432\n",
            "\n",
            "[[ 2.0111105]\n",
            " [-3.3594108]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 54, loss 0.005388\n",
            "\n",
            "[[ 2.0123613]\n",
            " [-3.361157 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 55, loss 0.005386\n",
            "\n",
            "[[ 2.0132592]\n",
            " [-3.363145 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 56, loss 0.005406\n",
            "\n",
            "[[ 2.014327 ]\n",
            " [-3.3643436]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 57, loss 0.005402\n",
            "\n",
            "[[ 2.0153193]\n",
            " [-3.366095 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 58, loss 0.005449\n",
            "\n",
            "[[ 2.0163786]\n",
            " [-3.3669372]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 59, loss 0.005396\n",
            "\n",
            "[[ 2.0185485]\n",
            " [-3.3672853]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 60, loss 0.005405\n",
            "\n",
            "[[ 2.019077 ]\n",
            " [-3.3683836]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 61, loss 0.005401\n",
            "\n",
            "[[ 2.019305 ]\n",
            " [-3.3692834]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 62, loss 0.005396\n",
            "\n",
            "[[ 2.0197275]\n",
            " [-3.3702533]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 63, loss 0.005385\n",
            "\n",
            "[[ 2.0206265]\n",
            " [-3.3709724]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 64, loss 0.005392\n",
            "\n",
            "[[ 2.021114 ]\n",
            " [-3.3718677]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 65, loss 0.005398\n",
            "\n",
            "[[ 2.0217237]\n",
            " [-3.3729532]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 66, loss 0.005403\n",
            "\n",
            "[[ 2.022183 ]\n",
            " [-3.3735228]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 67, loss 0.005497\n",
            "\n",
            "[[ 2.0226707]\n",
            " [-3.3740888]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 68, loss 0.005384\n",
            "\n",
            "[[ 2.0236857]\n",
            " [-3.3742268]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 69, loss 0.005386\n",
            "\n",
            "[[ 2.024063 ]\n",
            " [-3.3749328]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 70, loss 0.005388\n",
            "\n",
            "[[ 2.0244238]\n",
            " [-3.3755703]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 71, loss 0.005389\n",
            "\n",
            "[[ 2.0245388]\n",
            " [-3.3760698]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 72, loss 0.005396\n",
            "\n",
            "[[ 2.0249777]\n",
            " [-3.376187 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 73, loss 0.005383\n",
            "\n",
            "[[ 2.0251276]\n",
            " [-3.3765135]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 74, loss 0.005384\n",
            "\n",
            "[[ 2.025116 ]\n",
            " [-3.3769672]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 75, loss 0.005442\n",
            "\n",
            "[[ 2.0252783]\n",
            " [-3.3769517]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 76, loss 0.005383\n",
            "\n",
            "[[ 2.025057 ]\n",
            " [-3.3773038]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 77, loss 0.005390\n",
            "\n",
            "[[ 2.0256083]\n",
            " [-3.377823 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 78, loss 0.005400\n",
            "\n",
            "[[ 2.0255256]\n",
            " [-3.3781652]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 79, loss 0.005384\n",
            "\n",
            "[[ 2.0255363]\n",
            " [-3.3781803]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 80, loss 0.005384\n",
            "\n",
            "[[ 2.025487]\n",
            " [-3.378243]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 81, loss 0.005427\n",
            "\n",
            "[[ 2.025517 ]\n",
            " [-3.3783581]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 82, loss 0.005397\n",
            "\n",
            "[[ 2.0260465]\n",
            " [-3.378086 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 83, loss 0.005387\n",
            "\n",
            "[[ 2.0265772]\n",
            " [-3.3780587]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 84, loss 0.005395\n",
            "\n",
            "[[ 2.0265646]\n",
            " [-3.3781393]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 85, loss 0.005389\n",
            "\n",
            "[[ 2.0266445]\n",
            " [-3.3778918]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 86, loss 0.005393\n",
            "\n",
            "[[ 2.0261085]\n",
            " [-3.3779109]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 87, loss 0.005430\n",
            "\n",
            "[[ 2.0261953]\n",
            " [-3.3782275]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 88, loss 0.005424\n",
            "\n",
            "[[ 2.026117]\n",
            " [-3.378503]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 89, loss 0.005395\n",
            "\n",
            "[[ 2.0261917]\n",
            " [-3.3787358]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 90, loss 0.005541\n",
            "\n",
            "[[ 2.0260763]\n",
            " [-3.3789375]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 91, loss 0.005384\n",
            "\n",
            "[[ 2.0258243]\n",
            " [-3.3789907]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 92, loss 0.005384\n",
            "\n",
            "[[ 2.0257633]\n",
            " [-3.379105 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 93, loss 0.005392\n",
            "\n",
            "[[ 2.0254843]\n",
            " [-3.3789728]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 94, loss 0.005417\n",
            "\n",
            "[[ 2.0256853]\n",
            " [-3.379227 ]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 95, loss 0.005387\n",
            "\n",
            "[[ 2.0256567]\n",
            " [-3.3789523]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 96, loss 0.005384\n",
            "\n",
            "[[ 2.0258515]\n",
            " [-3.3789449]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 97, loss 0.005384\n",
            "\n",
            "[[ 2.025927 ]\n",
            " [-3.3791947]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 98, loss 0.005396\n",
            "\n",
            "[[ 2.0257938]\n",
            " [-3.3799012]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 99, loss 0.005399\n",
            "\n",
            "[[ 2.0260668]\n",
            " [-3.3794956]]\n",
            "<NDArray 2x1 @gpu(0)>\n",
            "epoch 100, loss 0.005391\n",
            "\n",
            "[[ 2.0260212]\n",
            " [-3.3797057]]\n",
            "<NDArray 2x1 @gpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hLoQ_2Ux_3l",
        "colab_type": "text"
      },
      "source": [
        "error in estimated weights vs actual weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw9yKoWHxtep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9b9ee256-aba2-4d9c-ebe1-f5fd3aea8d09"
      },
      "source": [
        "print('Error in estimating w', true_w - lrobj.weights.reshape(true_w.shape))\n",
        "print('Error in estimating b', true_b - lrobj.bias)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in estimating w \n",
            "[-0.02602124 -0.02029443]\n",
            "<NDArray 2 @gpu(0)>\n",
            "Error in estimating b \n",
            "[0.00297785]\n",
            "<NDArray 1 @gpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-mWWO9zybBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}